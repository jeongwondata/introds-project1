print("ANOVA did not produce valid results.")
}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))
# Install required packages if they are not already installed
required_packages <- c("sf", "ggplot2", "viridis", "dplyr", "lubridate")
new_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]
if (length(new_packages)) {
install.packages(new_packages)
}
# Load libraries
library(sf)
library(ggplot2)
library(viridis)
library(dplyr)
library(lubridate)
# Load necessary libraries
library(dplyr)
# Load the fire incidence CSV file
fire_data_path <- "C:/Users/SIMBAH/Desktop/Intro to Data science/Project/Fire incidences DC_VA_MD_cleaned.csv"  # Update with your actual path
fire_data <- read.csv(fire_data_path)
# Check the structure of the data
str(fire_data)
# Summarize the data to get counts of incidents by state and cause
incident_counts <- fire_data %>%
group_by(STATE, NWCG_GENERAL_CAUSE) %>%
summarise(Incident_Count = n(), .groups = 'drop')
# Check the summarized data
print(incident_counts)
# 1. ANOVA: Testing for differences in means across states based on cause
anova_result <- aov(Incident_Count ~ STATE, data = incident_counts)  # Adjust column names as necessary
anova_summary <- summary(anova_result)
# Print ANOVA summary for debugging
print(anova_summary)
# Check if ANOVA summary has results
if (length(anova_summary) > 0 && !is.null(anova_summary[[1]])) {
# Check if p-value is significant
if (anova_summary[[1]][["Pr(>F)"]][1] < 0.05) {
tukey_result <- TukeyHSD(anova_result)  # Perform Tukey's HSD test for pairwise comparisons
print(tukey_result)
} else {
print("No significant differences found in ANOVA.")
}
} else {
print("ANOVA did not produce valid results.")
}
# 2. t-test: Compare Virginia (VA) and Maryland (MD) means
va_incidents <- incident_counts %>% filter(STATE == "VA")  # Using STATE for Virginia
md_incidents <- incident_counts %>% filter(STATE == "MD")  # Using STATE for Maryland
# Ensure there are enough observations
if (nrow(va_incidents) > 0 && nrow(md_incidents) > 0) {
t_test_result <- t.test(va_incidents$Incident_Count, md_incidents$Incident_Count, var.equal = TRUE)  # Adjust as necessary
print(t_test_result)
} else {
print("Not enough data for t-test.")
}
# 3. Confidence Interval for Virginia's mean
if (nrow(va_incidents) > 0) {
mean_va <- mean(va_incidents$Incident_Count, na.rm = TRUE)
std_error_va <- sd(va_incidents$Incident_Count, na.rm = TRUE) / sqrt(nrow(va_incidents))
# 95% Confidence Interval
error_margin <- qt(0.975, df = nrow(va_incidents) - 1) * std_error_va
ci_va <- c(mean_va - error_margin, mean_va + error_margin)
print(paste("95% Confidence Interval for Virginia's mean:", ci_va[1], "to", ci_va[2]))
} else {
print("No data available for Virginia.")
}
# 1. ANOVA: Testing for differences in means across states based on cause
anova_result <- aov(Incident_Count ~ STATE, data = incident_counts)  # Adjust column names as necessary
anova_summary <- summary(anova_result)
# Print ANOVA summary for debugging
print(anova_summary)
# Check if ANOVA summary has results
if (length(anova_summary) > 0 && !is.null(anova_summary)) {
# Check if p-value is significant
if (anova_summary[[1]][["Pr(>F)"]][1] < 0.05) {
tukey_result <- TukeyHSD(anova_result)  # Perform Tukey's HSD test for pairwise comparisons
print(tukey_result)
} else {
print("No significant differences found in ANOVA.")
}
} else {
print("ANOVA did not produce valid results.")
}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))
# Install required packages if they are not already installed
required_packages <- c("sf", "ggplot2", "viridis", "dplyr", "lubridate")
new_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]
if (length(new_packages)) {
install.packages(new_packages)
}
# Load libraries
library(sf)
library(ggplot2)
library(viridis)
library(dplyr)
library(lubridate)
# Common R Markdown options
knitr::opts_chunk$set(results="markup", warning = F, message = F)
# Set options for number display format
options(scientific=T, digits = 3)
# Set global chunk options
knitr::opts_chunk$set(echo = FALSE, results = "markup", warning = FALSE, message = FALSE)
# Set options for number display format
options(scientific = TRUE, digits = 3)
The spatial pattern of fire incidents is a critical area of study, particularly in urban environments where population density and infrastructure can significantly influence fire dynamics. This research project focuses on the spatial distribution of fire incidents in Washington, D.C., Maryland, and Virginia, examining how geographic and environmental factors contribute to the occurrence and frequency of fires.
knitr::opts_chunk$set(echo = FALSE, results = "markup", warning = FALSE, message = FALSE)
# Specify the path to your study area shapefile
shapefile_path <- "C:/Users/SIMBAH/Desktop/Intro to Data science/Project/DC_MD_VA Boundaries/Project_Area.shp"
# Load the shapefile
Study_Area <- st_read(shapefile_path)
# Check the CRS
print(st_crs(Study_Area))
# Transform to NAD83 / UTM Zone 18N (EPSG:26918)
Study_Area <- st_transform(Study_Area, crs = 26918)
# View the structure of the shapefile data
#str(Study_Area)
# Plot the data with labels
library(ggplot2)
library(viridis)
ggplot(data = Study_Area) +
geom_sf(aes(fill = NAME_1), color = "Green") +  # Fill by NAME_1 and add a border
geom_sf_text(aes(label = NAME_1), size = 3, check_overlap = TRUE, color = "black", fontface = "bold") +  # Add labels in bold black
scale_fill_viridis_d(option = "C", name = "Key") +  # Update legend title to "Key"
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5),  # Center the title
legend.position = "right"  # Position the legend on the right
) +
ggtitle("Project Study Area Map")
required_packages <- c("sf", "ggplot2", "viridis", "dplyr", "lubridate")
new_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]
if (length(new_packages)) {
install.packages(new_packages)
}
# Load libraries
library(sf)
library(ggplot2)
library(viridis)
library(dplyr)
library(lubridate)
library(tigris)
library(readr)
# Read CSV
clean_df = '/Users/jeongwonyoo/Documents/GitHub/introds-project1/Fire incidences DC_VA_MD_cleaned.csv'
raw_df = '/Users/jeongwonyoo/Documents/GitHub/introds-project1/Wildfire_Dataset.csv'
library('readr')
clean_df = read_csv(clean_df)
raw_df = read_csv(raw_df)
# Make row_df columns names same as clean_df
names(raw_df) = toupper(names(raw_df))
options(tigris_use_cache = TRUE)
states <- tigris::states(cb = TRUE, year = 2023) %>%
st_transform(4326) %>%
select(STUSPS, NAME, geometry)
sf_raw <- st_as_sf(raw_df %>% filter(!is.na(LATITUDE), !is.na(LONGITUDE)),
coords = c("LONGITUDE","LATITUDE"), crs = 4326)
raw_with_state_poly <- st_join(sf_raw, states, join = st_within) %>%
st_drop_geometry() %>%
rename(STATE_FROM_CLEAN = STUSPS)
View(raw_with_state_poly)
rename(raw_with_state_ploy[c(STATE_FROM_CLEAN = STATE)]
library(sf)
library(ggplot2)
library(viridis)
library(dplyr)
library(lubridate)
library(tigris)
library(readr)
library(tidyr)
dp = '/Users/jeongwonyoo/Documents/GitHub/introds-project1/clean_object2_2018_DCMDVA.csv'
object2 = read_csv(dp)
str(object2)
View(object2)
View(object2)
library(corrplot)
install(corrplot)
installed.packages(corrplot)
install.packages('corrplot')
library(corrplot)
env_vars <- object2 %>% select(PR, RMAX, RMIN, SPH, SRAD, TMMN, TMMX, VS, FM100, FM1000, ERC, ETR, PET, VPD)
cor_mat <- cor(env_vars, use = "complete.obs")
corrplot(cor_mat, method = "color", tl.cex = 0.8)
cor_target <- cor(env_vars, object2$BI, use = "complete.obs")
sort(cor_target, decreasing = TRUE)
library(ggplot2)
ggplot(object2, aes(x = LONGITUDE, y = LATITUDE, fill = TMMX)) +
geom_tile() +
scale_fill_viridis_c() +
labs(title = "Maximum Temperature (°C) Distribution") +
theme_minimal()
ggplot(merged, aes(x = TMMX, y = BI)) +
geom_point(alpha = 0.4) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(title = "Fire Index vs. Temperature")
ggplot(object2, aes(x = TMMX, y = BI)) +
geom_point(alpha = 0.4) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(title = "Fire Index vs. Temperature")
model <- lm(BI ~ TMMX + PR + VS + VPD, data = obejct2)
model <- lm(BI ~ TMMX + PR + VS + VPD, data = object2)
par(mfrow = c(2, 2))
plot(model)  # residuals, Q-Q, leverage plots
library(spdep)
install.packages('spdep')
library(spdep)
coords <- cbind(object2$LONGITUDE, object2$LATITUDE)
nb <- knn2nb(knearneigh(coords, k = 5))
lw <- nb2listw(nb, style = "W")
library(dplyr)
library(spdep)
# 1) 스냅샷 날짜 선택
peak_day <- object2 %>%
group_by(DATETIME) %>%
summarise(mean_erc = mean(ERC, na.rm = TRUE)) %>%
slice_max(mean_erc, n = 1) %>%
pull(DATETIME)
snap <- filter(object2, DATETIME == peak_day)
# 2) 좌표 행렬
coords <- cbind(snap$LONGITUDE, snap$LATITUDE)
# 3) k를 늘려가며 연결성 확보
k <- 5
repeat {
nb <- knn2nb(knearneigh(coords, k = k, longlat = TRUE))
if (n.comp.nb(nb)$nc == 1 || k >= 20) break
k <- k + 1
}
lw <- nb2listw(nb, style = "W")
n.comp.nb(nb)$nc    # 1이면 OK
k                  # 최종 사용된 k 확인
# 0 ~ 150 km 사이 이웃, 지구곡면 거리 사용
nb <- dnearneigh(coords, d1 = 0, d2 = 150, longlat = TRUE)
# 고립점 처리
nb <- nb %>% include.self()  # 필요 시
lw <- nb2listw(nb, style = "W")
# 0 ~ 150 km 사이 이웃, 지구곡면 거리 사용
nb <- dnearneigh(coords, d1 = 0, d2 = 150, longlat = TRUE)
# 고립점 처리
nb <- nb %>% include.self()  # 필요 시
lw <- nb2listw(nb, style = "W")
ggplot(object2, aes(x = TMMX, y = BI)) +
+     geom_point(alpha = 0.4) +
+     geom_smooth(method = "lm", se = FALSE, color = "red") +
+     labs(title = "Fire Index vs. Temperature")
ggplot(object2, aes(x = TMMX, y = BI)) +geom_point(alpha = 0.4)+     geom_smooth(method = "lm", se = FALSE, color = "red")+labs(title = "Fire Index vs. Temperature")
# 0) 단면화: 같은 좌표의 여러 날짜를 평균으로 축약 (필요시 계절/기간 필터 후 평균)
o2 <- object2 %>%
group_by(LONGITUDE, LATITUDE) %>%
summarise(across(c(PR, RMAX, RMIN, SPH, SRAD, TMMN, TMMX, VS,
FM100, FM1000, ERC, ETR, PET, VPD),
~mean(.x, na.rm = TRUE)),
.groups = "drop")
# 1) 좌표 행렬 (지구곡면 거리 사용)
coords <- as.matrix(o2[, c("LONGITUDE","LATITUDE")])
# 2) 거리기반 이웃: 최소 하나 이웃 보장하는 적정 d2 찾기
#    (가장 가까운 이웃의 최대거리 + 약간의 여유)
knn1  <- knearneigh(coords, k = 1, longlat = TRUE)
dmax  <- max(knn1$nn.dist)
nb    <- dnearneigh(coords, d1 = 0, d2 = dmax * 1.05, longlat = TRUE)
# 3) 고립점 제거/연결성 확보
#    연결 컴포넌트 확인
if (n.comp.nb(nb)$nc > 1) {
# k-NN로 대체: k를 늘려가며 1개 컴포넌트 확보
k <- 5
repeat {
nb <- knn2nb(knearneigh(coords, k = k, longlat = TRUE))
if (n.comp.nb(nb)$nc == 1 || k >= 20) break
k <- k + 1
}
}
# 4) 가중치 행렬
lw <- nb2listw(nb, style = "W", zero.policy = TRUE)
# 5) 분석용 데이터: NA 제거 + (선택) 표준화
dat <- o2 %>%
select(ERC, TMMX, PR, VS, VPD, FM100, RMAX, SRAD) %>%  # 필요 변수 골라서
na.omit()
# nb/listw와 행 일치시키기 (na.omit으로 빠진 행 반영)
keep <- as.numeric(rownames(dat))
nb_k  <- subset.nb(nb, keep, sym = TRUE)
# 0) 단면 데이터(o2)와 nb는 '같은 행 순서' 기준이어야 함
#    이미 o2로 nb를 만들었다고 가정
#    회귀에 쓸 변수 목록
vars <- c("ERC","TMMX","PR","VS","VPD","FM100","RMAX","SRAD")
# 1) 결측 없는 행을 '논리형'으로 만들기
keep_logical <- complete.cases(o2[, vars])
# 2) 데이터와 이웃동시 서브셋
dat  <- o2[keep_logical, vars, drop = FALSE]
nb_k <- subset.nb(nb, keep_logical, sym = TRUE)
# 3) 가중치 행렬
lw_k <- nb2listw(nb_k, style = "W", zero.policy = TRUE)
# 4) OLS + Moran’s I
ols <- lm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD, data = dat)
lm.morantest(ols, lw_k, zero.policy = TRUE)
# 5) SAR/SEM
sar <- lagsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
# 현재 spdep 버전 확인(선택)
packageVersion("spdep")
# 1) spatialreg 설치
install.packages("spatialreg")              # CRAN
# 만약 실패하면 의존성 함께
# install.packages("spatialreg", dependencies = TRUE)
# 2) 로드
library(spatialreg)
# 3) 기존 코드 그대로 실행
sar <- lagsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
sem <- errorsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
summary(sar)
summary(sem)
AIC(sar, sem)
# 잔차의 공간자기상관 점검
lm.morantest(sar,  lw_k, zero.policy = TRUE)
# OLS에서 잔차의 공간자기상관
lm.morantest(ols, lw_k, zero.policy = TRUE)
# SAR/SEM에서는 잔차 벡터로 Moran's I
library(spdep)       # moran.test는 spdep에 있음
moran.test(residuals(sar), lw_k, zero.policy = TRUE)
moran.test(residuals(sem), lw_k, zero.policy = TRUE)
# 현재 spdep 버전 확인(선택)
packageVersion("spdep")
# 1) spatialreg 설치
install.packages("spatialreg")              # CRAN
# 현재 spdep 버전 확인(선택)
packageVersion("spdep")
# 1) spatialreg 설치
install.packages("spatialreg")              # CRAN
# 만약 실패하면 의존성 함께
# install.packages("spatialreg", dependencies = TRUE)
# 2) 로드
library(spatialreg)
# 3) 기존 코드 그대로 실행
sar <- lagsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
sem <- errorsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
summary(sar)
summary(sem)
AIC(sar, sem)
# OLS에서 잔차의 공간자기상관
lm.morantest(ols, lw_k, zero.policy = TRUE)
# If not already installed
# install.packages("spdep")
library(spdep)  # must be loaded for lm.morantest
spdep::lm.morantest(ols, lw_k, zero.policy = TRUE)
library(spatialreg)
library(spdep)
# Fit models
ols <- lm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD, data = dat)
sar <- lagsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
sem <- errorsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
# Summaries
summary(ols); summary(sar); summary(sem); AIC(sar, sem)
# Spatial autocorrelation of residuals
spdep::lm.morantest(ols, lw_k, zero.policy = TRUE)         # OLS
moran.test(residuals(sar), lw_k, zero.policy = TRUE)        # SAR
moran.test(residuals(sem), lw_k, zero.policy = TRUE)        # SEM
library(ggplot2)
ggplot(object2, aes(x = LONGITUDE, y = LATITUDE, fill = TMMX)) +
geom_tile() +
scale_fill_viridis_c() +
labs(title = "Maximum Temperature (°C) Distribution") +
theme_minimal()
library(dplyr)
library(ggplot2)
# 1) aggregate to one value per location (mean across dates)
grid_avg <- object2 %>%
group_by(LONGITUDE, LATITUDE) %>%
summarise(TMMX_K = mean(TMMX, na.rm = TRUE), .groups = "drop") %>%
mutate(TMMX_C = TMMX_K - 273.15)   # convert to °C (your values look like Kelvin)
# 2) draw tiles with an explicit size
ggplot(grid_avg, aes(x = LONGITUDE, y = LATITUDE, fill = TMMX_C)) +
geom_tile(width = 0.04, height = 0.04) +   # adjust size to your grid spacing
scale_fill_viridis_c(name = "TMMX (°C)") +
coord_fixed() +
labs(title = "Maximum Temperature Distribution") +
theme_minimal()
ggplot(object2, aes(LONGITUDE, LATITUDE, z = TMMX - 273.15)) +
stat_summary_2d(fun = mean, bins = 60) +   # mean in each 2D bin
scale_fill_viridis_c(name = "TMMX (°C)") +
coord_fixed() +
labs(title = "Maximum Temperature Distribution") +
theme_minimal()
ggplot(object2, aes(LONGITUDE, LATITUDE, color = TMMX - 273.15)) +
geom_point(size = 1, alpha = 0.7) +
scale_color_viridis_c(name = "TMMX (°C)") +
coord_fixed() +
labs(title = "Maximum Temperature (points)") +
theme_minimal()
library(ggplot2)
ggplot(object2, aes(x = LONGITUDE, y = LATITUDE, fill = TMMX)) +
geom_tile() +
scale_fill_viridis_c() +
labs(title = "Maximum Temperature (°C) Distribution") +
theme_minimal()
# 1) aggregate to one value per location (mean across dates)
grid_avg <- object2 %>%
group_by(LONGITUDE, LATITUDE) %>%
summarise(TMMX_K = mean(TMMX, na.rm = TRUE), .groups = "drop") %>%
mutate(TMMX_C = TMMX_K - 273.15)   # convert to °C (your values look like Kelvin)
# 2) draw tiles with an explicit size
ggplot(grid_avg, aes(x = LONGITUDE, y = LATITUDE, fill = TMMX_C)) +
geom_tile(width = 0.04, height = 0.04) +   # adjust size to your grid spacing
scale_fill_viridis_c(name = "TMMX (°C)") +
coord_fixed() +
labs(title = "Maximum Temperature Distribution") +
theme_minimal()
ggplot(object2, aes(LONGITUDE, LATITUDE, z = TMMX - 273.15)) +
stat_summary_2d(fun = mean, bins = 60) +   # mean in each 2D bin
scale_fill_viridis_c(name = "TMMX (°C)") +
coord_fixed() +
labs(title = "Maximum Temperature Distribution") +
theme_minimal()
ggplot(object2, aes(LONGITUDE, LATITUDE, z = TMMX - 273.15)) +
stat_summary_2d(fun = mean, bins = 80, na.rm = TRUE) +   # try 40, 60, 100
coord_fixed(expand = FALSE) +
scale_fill_viridis_c(name = "TMMX (°C)") +
labs(title = "Maximum Temperature Distribution") +
theme_minimal(base_size = 12)
ggplot(object2, aes(LONGITUDE, LATITUDE, z = TMMX - 273.15)) +
stat_summary_2d(fun = mean, binwidth = c(0.15, 0.15), na.rm = TRUE) +  # set lon/lat step
coord_fixed(expand = FALSE) +
scale_fill_viridis_c(name = "TMMX (°C)") +
labs(title = "Maximum Temperature Distribution") +
theme_minimal(base_size = 12)
library(dplyr); library(ggplot2)
plot_raster <- function(df, var, bins = 60, to_celsius = FALSE) {
z <- if (to_celsius) df[[var]] - 273.15 else df[[var]]
ggplot(df, aes(LONGITUDE, LATITUDE, z = z)) +
stat_summary_2d(fun = mean, bins = bins, na.rm = TRUE) +
coord_fixed(expand = FALSE) +
scale_fill_viridis_c(name = var) +
labs(title = paste(var, "Distribution")) +
theme_minimal(base_size = 12)
}
# examples
plot_raster(object2, "TMMX", bins = 70, to_celsius = TRUE)
plot_raster(object2, "VPD",  bins = 70)
plot_raster(object2, "FM100", bins = 70)
plot_raster(object2, "PR",   bins = 70)
plot_raster(object2, "VS",   bins = 70)
plot_raster(object2, "ERC",  bins = 70)
# Fit models
ols <- lm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD, data = dat)
sar <- lagsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
sem <- errorsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat, listw = lw_k, zero.policy = TRUE, na.action = na.exclude)
# Summaries
summary(ols); summary(sar); summary(sem); AIC(sar, sem)
# Spatial autocorrelation of residuals
spdep::lm.morantest(ols, lw_k, zero.policy = TRUE)         # OLS
moran.test(residuals(sar), lw_k, zero.policy = TRUE)        # SAR
moran.test(residuals(sem), lw_k, zero.policy = TRUE)        # SEM
o2 <- object2 %>%
group_by(LONGITUDE, LATITUDE) %>%
summarise(across(c(PR, RMAX, RMIN, SPH, SRAD, TMMN, TMMX, VS,
FM100, FM1000, ERC, ETR, PET, VPD),
~mean(.x, na.rm = TRUE)),
.groups = "drop")
vars <- c("ERC","TMMX","VPD","FM100","PR","VS","RMAX","SRAD")
dat  <- na.omit(o2[, vars, drop = FALSE])
coords_k <- as.matrix(
o2[as.numeric(rownames(dat)), c("LONGITUDE","LATITUDE")]
)
kn1  <- knearneigh(coords_k, k = 1, longlat = TRUE)
dmax <- max(kn1$nn.dist)
nb_k <- dnearneigh(coords_k, 0, dmax*1.05, longlat = TRUE)
while (any(card(nb_k) == 0)) { dmax <- dmax*1.2; nb_k <- dnearneigh(coords_k, 0, dmax, longlat = TRUE) }
# 0) 좌표별 평균으로 단면화 + id 부여
o2 <- object2 %>%
group_by(LONGITUDE, LATITUDE) %>%
summarise(across(c(PR, RMAX, RMIN, SPH, SRAD, TMMN, TMMX, VS,
FM100, FM1000, ERC, ETR, PET, VPD),
~mean(.x, na.rm = TRUE)), .groups = "drop") %>%
mutate(id = row_number())
# 1) 모델 변수 선택
vars <- c("ERC","TMMX","VPD","FM100","PR","VS","RMAX","SRAD")
# 2) 완전케이스 마스크를 '논리 벡터'로 만들고, id도 같이 유지
keep_logical <- complete.cases(o2[, vars])
# 3) 분석 데이터
dat <- o2[keep_logical, c(vars, "id"), drop = FALSE]
stopifnot(nrow(dat) > 1)  # 안전 체크: 최소 2행 이상이어야 이웃 가능
# 4) 좌표행렬은 id로 동기화 (rownames 사용 금지)
coords <- as.matrix(o2[, c("LONGITUDE","LATITUDE")])
coords_k <- coords[dat$id, , drop = FALSE]
stopifnot(nrow(coords_k) == nrow(dat))
# 5) k-NN 이웃으로 바로 생성하여 연결성 확보
k <- 8
repeat {
nb <- knn2nb(knearneigh(coords_k, k = k, longlat = TRUE))
if (n.comp.nb(nb)$nc == 1 || k >= 20) break
k <- k + 2
}
# 가중치 행렬
lw_k <- nb2listw(nb, style = "W", zero.policy = FALSE)
# 6) 예측변수 표준화(수렴 안정)
xvars <- c("TMMX","VPD","FM100","PR","VS","RMAX","SRAD")
dat_std <- dat
dat_std[xvars] <- scale(dat_std[xvars])
# 7) 모델
ols <- lm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD, data = dat_std)
sem <- errorsarlm(ERC ~ TMMX + VPD + FM100 + PR + VS + RMAX + SRAD,
data = dat_std, listw = lw_k, na.action = na.exclude)
summary(ols)
summary(sem)
moran.test(residuals(ols), lw_k)
moran.test(residuals(sem), lw_k)
